{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import pandas as pd\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "import hdbscan\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from typing import Dict, List  # Ensure this is imported\n",
        "\n",
        "\n",
        "# Step 1: Load the Qwen Model and Tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-1.5B-Instruct\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen2.5-1.5B-Instruct\")\n",
        "\n",
        "# Step 2: Load and Chunk Dataset\n",
        "def load_and_chunk_datasets(folder_path: str) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Load all JSON datasets and chunk data for embedding and clustering.\n",
        "    \"\"\"\n",
        "    data = []\n",
        "    for file in os.listdir(folder_path):\n",
        "        if file.endswith('.json'):\n",
        "            with open(os.path.join(folder_path, file), 'r') as f:\n",
        "                content = json.load(f)\n",
        "                name = content.get(\"name\", \"\")\n",
        "                uses = content.get(\"uses\", \"\")\n",
        "                side_effects = content.get(\"side_effects\", \"\")\n",
        "                dosage = content.get(\"dosage\", \"\")\n",
        "\n",
        "                # Chunk into smaller pieces\n",
        "                chunks = [\n",
        "                    {\"text\": f\"Name: {name}. Uses: {uses}\", \"source\": file},\n",
        "                    {\"text\": f\"Side Effects: {side_effects}\", \"source\": file},\n",
        "                    {\"text\": f\"Dosage: {dosage}\", \"source\": file},\n",
        "                ]\n",
        "                data.extend(chunks)\n",
        "    return pd.DataFrame(data)\n",
        "\n",
        "# Load and chunk dataset\n",
        "dataset_path = '/kaggle/input/random/datasets/microlabs_usa'\n",
        "pharma_data = load_and_chunk_datasets(dataset_path)\n",
        "\n",
        "# Step 3: Generate BERT Embeddings\n",
        "bert_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "pharma_data['embedding'] = pharma_data['text'].apply(lambda x: bert_model.encode(x))\n",
        "\n",
        "# Step 4: Clustering with HDBSCAN\n",
        "embeddings = list(pharma_data['embedding'])\n",
        "clusterer = hdbscan.HDBSCAN(min_cluster_size=5, metric='euclidean')\n",
        "pharma_data['cluster'] = clusterer.fit_predict(embeddings)\n",
        "\n",
        "# Step 5: Calculate Cluster Centroids\n",
        "cluster_centroids = (\n",
        "    pharma_data.groupby('cluster')['embedding']\n",
        "    .apply(lambda x: sum(x) / len(x))\n",
        "    .to_dict()\n",
        ")\n",
        "\n",
        "# Step 6: Nearest Neighbor Search\n",
        "# Fit NearestNeighbors for retrieval within clusters\n",
        "clustered_data = pharma_data[pharma_data['cluster'] != -1]  # Exclude noise\n",
        "nn_models = {}\n",
        "for cluster_id in clustered_data['cluster'].unique():\n",
        "    cluster_subset = clustered_data[clustered_data['cluster'] == cluster_id]\n",
        "    nn = NearestNeighbors(n_neighbors=5, metric='euclidean')\n",
        "    nn.fit(list(cluster_subset['embedding']))\n",
        "    nn_models[cluster_id] = (nn, cluster_subset)\n",
        "\n",
        "# Step 7: RAG-Powered Assistant\n",
        "class PharmaKnowledgeAssistant:\n",
        "    def _init_(self, data: pd.DataFrame, nn_models: Dict, bert_model, cluster_centroids, tokenizer, model):\n",
        "        self.data = data\n",
        "        self.nn_models = nn_models\n",
        "        self.bert_model = bert_model\n",
        "        self.cluster_centroids = cluster_centroids\n",
        "        self.tokenizer = tokenizer\n",
        "        self.model = model\n",
        "\n",
        "    def retrieve_relevant_chunks(self, query: str) -> List[Dict]:\n",
        "        \"\"\"\n",
        "        Retrieve relevant chunks using BERT embeddings and nearest neighbor search.\n",
        "        \"\"\"\n",
        "        query_embedding = self.bert_model.encode(query)\n",
        "\n",
        "        # Find the nearest cluster centroid\n",
        "        cluster_distances = {\n",
        "            cluster_id: sum((query_embedding - centroid) ** 2)\n",
        "            for cluster_id, centroid in self.cluster_centroids.items()\n",
        "        }\n",
        "        best_cluster = min(cluster_distances, key=cluster_distances.get)\n",
        "\n",
        "        # Retrieve nearest neighbors within the best cluster\n",
        "        nn, cluster_subset = self.nn_models[best_cluster]\n",
        "        distances, indices = nn.kneighbors([query_embedding])\n",
        "        relevant_chunks = cluster_subset.iloc[indices[0]].to_dict(orient='records')\n",
        "        return relevant_chunks\n",
        "\n",
        "    def generate_response(self, query: str, context_chunks: List[Dict]) -> str:\n",
        "        \"\"\"\n",
        "        Generate a response using the Qwen 2.5-1.5B-Instruct model, augmented with retrieved context.\n",
        "        \"\"\"\n",
        "        context_text = \"\\n\".join([chunk['text'] for chunk in context_chunks])\n",
        "        prompt = (\n",
        "            f\"Context:\\n{context_text}\\n\\n\"\n",
        "            f\"User Query:\\n{query}\\n\\n\"\n",
        "            \"Answer:\"\n",
        "        )\n",
        "        try:\n",
        "            inputs = self.tokenizer(prompt, return_tensors=\"pt\")\n",
        "            outputs = self.model.generate(inputs.input_ids, max_length=200, num_return_sequences=1)\n",
        "            return self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        except Exception as e:\n",
        "            return f\"Error generating response: {str(e)}\"\n",
        "\n",
        "    def recommend_safe_medication(self, symptom: str, condition: str) -> str:\n",
        "        \"\"\"\n",
        "        Generate medication recommendations based on symptoms and conditions dynamically.\n",
        "        \"\"\"\n",
        "        # Filter the dataset for medications relevant to the symptom and condition\n",
        "        relevant_data = self.data[\n",
        "            self.data['text'].str.contains(symptom, case=False) &\n",
        "            self.data['text'].str.contains(condition, case=False)\n",
        "        ]\n",
        "\n",
        "        if relevant_data.empty:\n",
        "            return \"No suitable medication found for the given symptoms and conditions.\"\n",
        "\n",
        "        # If relevant data is found, generate a list of medications\n",
        "        recommended_medications = []\n",
        "        for _, row in relevant_data.iterrows():\n",
        "            text = row['text']\n",
        "            source = row['source']\n",
        "            recommended_medications.append(f\"Product from {source}: {text}\")\n",
        "\n",
        "        return \"\\n\".join(recommended_medications)\n",
        "\n",
        "    def answer_query(self, query: str) -> str:\n",
        "        \"\"\"\n",
        "        Answer the user's query using RAG or external generation.\n",
        "        \"\"\"\n",
        "        if \"symptom:\" in query.lower() and \"condition:\" in query.lower():\n",
        "            # Handle symptom-condition recommendation\n",
        "            parts = query.split(\"condition:\")\n",
        "            symptom = parts[0].replace(\"symptom:\", \"\").strip()\n",
        "            condition = parts[1].strip()\n",
        "            return self.recommend_safe_medication(symptom, condition)\n",
        "\n",
        "        # Retrieve relevant chunks for general queries\n",
        "        relevant_chunks = self.retrieve_relevant_chunks(query)\n",
        "        return self.generate_response(query, relevant_chunks)\n",
        "\n",
        "# Step 8: Interactive Assistant\n",
        "def interactive_assistant():\n",
        "    \"\"\"\n",
        "    Main interactive loop for the assistant.\n",
        "    \"\"\"\n",
        "    assistant = PharmaKnowledgeAssistant(pharma_data, nn_models, bert_model, cluster_centroids, tokenizer, model)\n",
        "    print(\"Welcome to the Pharma Knowledge Assistant! Type 'exit' to quit.\")\n",
        "\n",
        "    while True:\n",
        "        query = input(\"Your Query: \")\n",
        "        if query.lower() == 'exit':\n",
        "            print(\"Goodbye!\")\n",
        "            break\n",
        "\n",
        "        response = assistant.answer_query(query)\n",
        "        print(\"\\nAssistant Response:\")\n",
        "        print(response)\n",
        "\n",
        "# Run the assistant\n",
        "interactive_assistant()"
      ],
      "metadata": {
        "id": "BrGRbwptVX2Y"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}